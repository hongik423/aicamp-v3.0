/**
 * í†µí•© AI í”„ë¡œë°”ì´ë” (Llama ìš°ì„ )
 * - ì§€ì›: Ollama(ë¡œì»¬), Groq, Together, OpenRouter, Gemini(í˜¸í™˜ ìœ ì§€)
 * - ì™¸ë¶€ SDK ì—†ì´ fetch ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
 */

export type AIProvider = 'ollama' | 'groq' | 'together' | 'openrouter' | 'gemini';

export interface ChatHistoryItem {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface CallAIParams {
  prompt?: string;
  system?: string;
  history?: ChatHistoryItem[];
  temperature?: number;
  maxTokens?: number;
  model?: string;
  timeoutMs?: number;
}

function getEnv(name: string, defaultValue: string = ''): string {
  return process.env[name] || defaultValue;
}

function getProvider(): AIProvider {
  const provider = (getEnv('AI_PROVIDER', 'ollama') as AIProvider);
  return provider;
}

function getModelForProvider(provider: AIProvider, override?: string): string {
  if (override) return override;
  switch (provider) {
    case 'ollama':
      return getEnv('OLLAMA_MODEL', 'gpt-oss:20b');
    case 'groq':
      return getEnv('GROQ_MODEL', 'llama-3.1-70b-versatile');
    case 'together':
      return getEnv('TOGETHER_MODEL', 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo');
    case 'openrouter':
      return getEnv('OPENROUTER_MODEL', 'meta-llama/llama-3.1-70b-instruct');
    case 'gemini':
      return getEnv('GEMINI_MODEL', 'gemini-2.5-flash-exp');
    default:
      return 'gpt-oss:20b';
  }
}

function withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {
  const controller = new AbortController();
  const timer = setTimeout(() => controller.abort(), ms);
  return new Promise<T>((resolve, reject) => {
    promise
      .then((value) => {
        clearTimeout(timer);
        resolve(value);
      })
      .catch((err) => {
        clearTimeout(timer);
        reject(err);
      });
  });
}

function normalizeHistoryToText(history?: ChatHistoryItem[], system?: string, userPrompt?: string): string {
  const parts: string[] = [];
  if (system && system.trim().length > 0) {
    parts.push(`[System]\n${system.trim()}`);
  }
  if (history && history.length > 0) {
    for (const h of history) {
      parts.push(`[${h.role}]\n${h.content}`);
    }
  }
  if (userPrompt && userPrompt.trim().length > 0) {
    parts.push(`[user]\n${userPrompt.trim()}`);
  }
  return parts.join('\n\n');
}

export async function callAI(params: CallAIParams): Promise<string> {
  const provider = getProvider();
  const temperature = params.temperature ?? 0.7;
  const maxTokens = params.maxTokens ?? 8192; // GPT-OSS 20B ìµœì í™”ëœ í† í° ìˆ˜
  const timeoutMs = params.timeoutMs ?? 900000; // 15ë¶„ (Ollama ìµœì í™”)
  const model = getModelForProvider(provider, params.model);
  
  console.log(`ğŸ¤– Ollama GPT-OSS 20B AI í˜¸ì¶œ: ${provider}/${model} (í† í°: ${maxTokens}, ì˜¨ë„: ${temperature})`);

  // Ollama ìš°ì„  ì‹œë„, ì‹¤íŒ¨ ì‹œ Gemini í´ë°±
  try {
    switch (provider) {
      case 'ollama':
        return await withTimeout(callOllama({ ...params, model, temperature, maxTokens }), timeoutMs);
      case 'groq':
        return await withTimeout(callOpenAICompatible({ ...params, model, temperature, maxTokens }, 'https://api.groq.com/openai/v1', getEnv('GROQ_API_KEY')), timeoutMs);
      case 'together':
        return await withTimeout(callOpenAICompatible({ ...params, model, temperature, maxTokens }, 'https://api.together.xyz/v1', getEnv('TOGETHER_API_KEY')), timeoutMs);
      case 'openrouter':
        return await withTimeout(callOpenAICompatible({ ...params, model, temperature, maxTokens }, 'https://openrouter.ai/api/v1', getEnv('OPENROUTER_API_KEY')), timeoutMs);
      case 'gemini':
        return await withTimeout(callGemini({ ...params, model, temperature, maxTokens }), timeoutMs);
      default:
        return await withTimeout(callOllama({ ...params, model, temperature, maxTokens }), timeoutMs);
    }
  } catch (error) {
    console.warn(`${provider} í˜¸ì¶œ ì‹¤íŒ¨, Geminië¡œ í´ë°±:`, error);
    // ëª¨ë“  í”„ë¡œë°”ì´ë” ì‹¤íŒ¨ ì‹œ Gemini í´ë°±
    return await withTimeout(callGemini({ 
      ...params, 
      model: 'gemini-2.5-flash-exp', 
      temperature, 
      maxTokens 
    }), 600000); // GeminiëŠ” 10ë¶„ íƒ€ì„ì•„ì›ƒ
  }
}

async function callOllama(params: Required<Pick<CallAIParams, 'model' | 'temperature' | 'maxTokens'>> & CallAIParams): Promise<string> {
  const apiUrl = getEnv('OLLAMA_API_URL', 'http://localhost:11434');
  const model = params.model || getEnv('OLLAMA_MODEL', 'gpt-oss:20b');
  
  if (!apiUrl) {
    throw new Error('OLLAMA_API_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }

  const prompt = normalizeHistoryToText(params.history, params.system, params.prompt);
  
  console.log(`ğŸš€ Ollama GPT-OSS 20B í˜¸ì¶œ ì‹œì‘: ${model}`);
  
  const res = await fetch(`${apiUrl}/api/generate`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: model,
      prompt: prompt,
      stream: false,
      options: {
        temperature: params.temperature,
        num_predict: params.maxTokens,
        top_k: 40,
        top_p: 0.95,
        repeat_penalty: 1.1,
        stop: ["<|im_end|>", "<|endoftext|>", "Human:", "Assistant:"],
        // GPT-OSS 20B ìµœì í™” ì„¤ì •
        num_ctx: 32768, // ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™•ì¥
        num_thread: 8,   // ë©€í‹°ìŠ¤ë ˆë”© ìµœì í™”
        num_gpu: 1       // GPU ê°€ì† í™œìš©
      }
    }),
    signal: AbortSignal.timeout(900000) // 15ë¶„ íƒ€ì„ì•„ì›ƒ
  });

  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`Ollama GPT-OSS 20B Error ${res.status}: ${txt}`);
  }
  
  const json = await res.json();
  const response = json.response || '';
  
  console.log(`âœ… Ollama GPT-OSS 20B ì‘ë‹µ ì™„ë£Œ: ${response.length} ë¬¸ì`);
  return response;
}

async function callOpenAICompatible(params: Required<Pick<CallAIParams, 'model' | 'temperature' | 'maxTokens'>> & CallAIParams, baseUrl: string, apiKey?: string): Promise<string> {
  if (!apiKey) {
    throw new Error('OpenAI í˜¸í™˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }
  const url = `${baseUrl.replace(/\/$/, '')}/chat/completions`;
  const messages: Array<{ role: string; content: string }> = [];
  if (params.system) messages.push({ role: 'system', content: params.system });
  if (params.history) {
    for (const h of params.history) messages.push({ role: h.role, content: h.content });
  }
  if (params.prompt) messages.push({ role: 'user', content: params.prompt });

  const res = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: params.model,
      temperature: params.temperature,
      max_tokens: params.maxTokens,
      messages
    })
  });
  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`OpenAI compatible Error ${res.status}: ${txt}`);
  }
  const json = await res.json();
  const text = json.choices?.[0]?.message?.content || '';
  return text;
}



async function callGemini(params: Required<Pick<CallAIParams, 'model' | 'temperature' | 'maxTokens'>> & CallAIParams): Promise<string> {
  const apiKey = getEnv('GEMINI_API_KEY');
  const apiUrl = getEnv('GEMINI_API_URL', 'https://generativelanguage.googleapis.com/v1beta/models');
  if (!apiKey) {
    throw new Error('GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }
  const modelPath = `${apiUrl.replace(/\/$/, '')}/${params.model}:generateContent`;
  const text = normalizeHistoryToText(params.history, params.system, params.prompt);
  const res = await fetch(`${modelPath}?key=${apiKey}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      contents: [{ parts: [{ text }] }],
      generationConfig: {
        temperature: params.temperature,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: params.maxTokens
      },
      safetySettings: [
        { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' }
      ]
    })
  });

  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`Gemini Error ${res.status}: ${txt}`);
  }
  const json = await res.json();
  return json.candidates?.[0]?.content?.parts?.[0]?.text || '';
}

export function extractJsonFromText(text: string): any | null {
  try {
    const jsonFence = text.match(/```json\s*([\s\S]*?)\s*```/);
    const candidate = jsonFence ? jsonFence[1] : text;
    return JSON.parse(candidate);
  } catch {
    return null;
  }
}


