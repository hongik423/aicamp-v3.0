/**
 * GPU ìµœì í™” ì‹œìŠ¤í…œ - ì´êµì¥ì˜AIìƒë‹´ ì „ìš©
 * NVIDIA GPU + NPU ìµœëŒ€ ì„±ëŠ¥ í™œìš©
 * phi3:mini ëª¨ë¸ ìµœì í™”
 */

export interface GPUOptimizationConfig {
  numGpu: number;
  numThread: number;
  numBatch: number;
  contextSize: number;
  useMlock: boolean;
  useMmap: boolean;
  numa: boolean;
  flashAttention: boolean;
  lowVram: boolean;
  gpuLayers: number;
  npuLayers: number;
  hybridMode: boolean;
  workloadDistribution?: { gpu: number; npu: number; cpu: number };
}

export interface SystemResources {
  totalMemory: number;
  availableMemory: number;
  cpuCores: number;
  gpuMemory?: number;
  hasNPU: boolean;
  gpuModel?: string;
  gpuUtilization?: number;
  gpuTemperature?: number;
}

export interface PerformanceMetrics {
  processingTime: number;
  tokensPerSecond: number;
  memoryUsage: number;
  gpuUtilization?: number;
  temperature?: number;
  throughput: number;
  latency: number;
}

/**
 * ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ë™ì  ìµœì í™” ì„¤ì • ìƒì„±
 */
export function generateOptimalConfig(
  resources: SystemResources,
  taskComplexity: 'simple' | 'medium' | 'complex' = 'medium'
): GPUOptimizationConfig {
  // phi3:mini ëª¨ë¸ì— ìµœì í™”ëœ ì„¤ì •
  const baseConfig: GPUOptimizationConfig = {
    numGpu: -1, // ëª¨ë“  GPU í™œìš©
    numThread: Math.min(resources.cpuCores, 16), // phi3:miniëŠ” ë” ì ì€ ìŠ¤ë ˆë“œë¡œ ì¶©ë¶„
    numBatch: 512, // phi3:miniì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
    contextSize: 32768, // phi3:mini ìµœì  ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
    useMlock: true,
    useMmap: true,
    numa: true,
    flashAttention: true,
    lowVram: false,
    gpuLayers: 20, // phi3:miniì— ìµœì í™”ëœ GPU ë ˆì´ì–´
    npuLayers: resources.hasNPU ? 6 : 0, // NPU ë ˆì´ì–´ ì¡°ì •
    hybridMode: resources.hasNPU
  };

  // GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¡°ì • (phi3:mini ìµœì í™”)
  if (resources.gpuMemory) {
    if (resources.gpuMemory < 4) {
      // 4GB ë¯¸ë§Œ: ìµœì†Œ ì„¤ì •
      baseConfig.gpuLayers = 8;
      baseConfig.numBatch = 256;
      baseConfig.contextSize = 16384;
      baseConfig.lowVram = true;
      baseConfig.workloadDistribution = { gpu: 40, npu: 30, cpu: 30 };
    } else if (resources.gpuMemory < 8) {
      // 4-8GB: ë³´ìˆ˜ì  ì„¤ì •
      baseConfig.gpuLayers = 12;
      baseConfig.numBatch = 384;
      baseConfig.contextSize = 24576;
      baseConfig.lowVram = true;
      baseConfig.workloadDistribution = { gpu: 50, npu: 35, cpu: 15 };
    } else if (resources.gpuMemory >= 16) {
      // 16GB ì´ìƒ: ìµœëŒ€ ì„±ëŠ¥ ì„¤ì •
      baseConfig.gpuLayers = 28;
      baseConfig.numBatch = 768;
      baseConfig.contextSize = 49152;
      baseConfig.workloadDistribution = { gpu: 70, npu: 20, cpu: 10 };
    } else {
      // 8-16GB: ê· í˜• ì„¤ì •
      baseConfig.gpuLayers = 20;
      baseConfig.numBatch = 512;
      baseConfig.contextSize = 32768;
      baseConfig.workloadDistribution = { gpu: 60, npu: 25, cpu: 15 };
    }
  }

  // ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì • (phi3:mini ìµœì í™”)
  switch (taskComplexity) {
    case 'simple':
      baseConfig.numBatch = 256;
      baseConfig.contextSize = 16384;
      baseConfig.gpuLayers = Math.max(6, baseConfig.gpuLayers - 4);
      break;
    case 'complex':
      baseConfig.numBatch = 768;
      baseConfig.contextSize = 49152;
      baseConfig.numThread = Math.min(resources.cpuCores * 2, 32);
      baseConfig.gpuLayers = Math.min(28, baseConfig.gpuLayers + 4);
      break;
  }

  // NPU í™œìš© ìµœì í™” (phi3:mini ìµœì í™”)
  if (resources.hasNPU) {
    baseConfig.numThread = Math.min(resources.cpuCores * 2, 48); // phi3:miniì— ìµœì í™”ëœ ìŠ¤ë ˆë“œ
    baseConfig.hybridMode = true;
    console.log('ğŸ§  NPU ê°ì§€: phi3:mini ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” í™œì„±í™”');
  }

  return baseConfig;
}

/**
 * ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
 */
export class GPUPerformanceMonitor {
  private metrics: PerformanceMetrics[] = [];
  private maxHistory = 100;
  private alerts: string[] = [];

  addMetric(metric: PerformanceMetrics) {
    this.metrics.push({
      ...metric,
      timestamp: Date.now()
    } as PerformanceMetrics & { timestamp: number });

    // íˆìŠ¤í† ë¦¬ ì œí•œ
    if (this.metrics.length > this.maxHistory) {
      this.metrics.shift();
    }

    // ì„±ëŠ¥ ê²½ê³  ì²´í¬
    this.checkPerformanceAlerts(metric);
  }

  private checkPerformanceAlerts(metric: PerformanceMetrics) {
    if (metric.gpuUtilization && metric.gpuUtilization > 95) {
      this.alerts.push(`GPU ì‚¬ìš©ë¥  ë†’ìŒ: ${Math.round(metric.gpuUtilization)}%`);
    }
    
    if (metric.temperature && metric.temperature > 80) {
      this.alerts.push(`GPU ì˜¨ë„ ë†’ìŒ: ${Math.round(metric.temperature)}Â°C`);
    }
    
    if (metric.memoryUsage > 0.9) {
      this.alerts.push(`ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: ${Math.round(metric.memoryUsage * 100)}%`);
    }
    
    if (metric.tokensPerSecond < 100) {
      this.alerts.push(`ì²˜ë¦¬ ì†ë„ ëŠë¦¼: ${Math.round(metric.tokensPerSecond)} tokens/sec`);
    }
  }

  getMetrics(): PerformanceMetrics[] {
    return [...this.metrics];
  }

  getAlerts(): string[] {
    return [...this.alerts];
  }

  clearAlerts(): void {
    this.alerts = [];
  }

  getAverageMetrics(): PerformanceMetrics {
    if (this.metrics.length === 0) {
      return {
        processingTime: 0,
        tokensPerSecond: 0,
        memoryUsage: 0,
        throughput: 0,
        latency: 0
      };
    }

    const sum = this.metrics.reduce((acc, metric) => ({
      processingTime: acc.processingTime + metric.processingTime,
      tokensPerSecond: acc.tokensPerSecond + metric.tokensPerSecond,
      memoryUsage: acc.memoryUsage + metric.memoryUsage,
      gpuUtilization: (acc.gpuUtilization || 0) + (metric.gpuUtilization || 0),
      temperature: (acc.temperature || 0) + (metric.temperature || 0),
      throughput: acc.throughput + metric.throughput,
      latency: acc.latency + metric.latency
    }), {
      processingTime: 0,
      tokensPerSecond: 0,
      memoryUsage: 0,
      throughput: 0,
      latency: 0
    });

    const count = this.metrics.length;
    return {
      processingTime: sum.processingTime / count,
      tokensPerSecond: sum.tokensPerSecond / count,
      memoryUsage: sum.memoryUsage / count,
      gpuUtilization: sum.gpuUtilization / count,
      temperature: sum.temperature / count,
      throughput: sum.throughput / count,
      latency: sum.latency / count
    };
  }

  generateReport(): string {
    const avgMetrics = this.getAverageMetrics();
    const alerts = this.getAlerts();
    
    return `
ğŸ® GPU ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸:
   âš¡ í‰ê·  ì²˜ë¦¬ì‹œê°„: ${Math.round(avgMetrics.processingTime)}ms
   ğŸš€ í‰ê·  ì²˜ë¦¬ì†ë„: ${Math.round(avgMetrics.tokensPerSecond)} tokens/sec
   ğŸ’¾ í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ${Math.round(avgMetrics.memoryUsage * 100)}%
   ğŸ“Š í‰ê·  GPU ì‚¬ìš©ë¥ : ${Math.round(avgMetrics.gpuUtilization || 0)}%
   ğŸŒ¡ï¸ í‰ê·  GPU ì˜¨ë„: ${Math.round(avgMetrics.temperature || 0)}Â°C
   ğŸ”„ í‰ê·  ì²˜ë¦¬ëŸ‰: ${Math.round(avgMetrics.throughput)} ops/sec
   â±ï¸ í‰ê·  ì§€ì—°ì‹œê°„: ${Math.round(avgMetrics.latency)}ms
   
${alerts.length > 0 ? `ğŸš¨ ì„±ëŠ¥ ê²½ê³ :\n${alerts.map(alert => `   - ${alert}`).join('\n')}` : 'âœ… ì„±ëŠ¥ ìƒíƒœ ì–‘í˜¸'}
    `;
  }
}

/**
 * GPU ìƒíƒœ í™•ì¸
 */
export async function checkGPUHealth(): Promise<{
  utilization: number;
  temperature: number;
  memoryUsed: number;
  memoryTotal: number;
  powerDraw: number;
  isAvailable: boolean;
}> {
  try {
    // GPU ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” nvidia-smi ë˜ëŠ” GPU API ì‚¬ìš©)
    const health = {
      utilization: Math.random() * 40 + 20, // 20-60%
      temperature: Math.random() * 20 + 50, // 50-70Â°C
      memoryUsed: Math.random() * 4 + 2,    // 2-6GB
      memoryTotal: 6,                       // 6GB RTX 4050
      powerDraw: Math.random() * 30 + 50,   // 50-80W
      isAvailable: true
    };
    
    return health;
  } catch (error) {
    console.warn('GPU ìƒíƒœ í™•ì¸ ì‹¤íŒ¨:', error);
    return {
      utilization: 0,
      temperature: 0,
      memoryUsed: 0,
      memoryTotal: 0,
      powerDraw: 0,
      isAvailable: false
    };
  }
}

/**
 * ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
 */
export function getOptimalBatchSize(
  gpuMemoryBytes: number, 
  maxTokens: number, 
  modelSizeGB: number
): number {
  const gpuMemoryGB = gpuMemoryBytes / (1024 * 1024 * 1024);
  const availableMemory = gpuMemoryGB * 0.8; // 80% ì‚¬ìš©
  
  // ëª¨ë¸ í¬ê¸°ì™€ í† í° ìˆ˜ë¥¼ ê³ ë ¤í•œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
  const baseBatchSize = Math.floor(availableMemory / (modelSizeGB * 0.1));
  const tokenBasedBatchSize = Math.floor(maxTokens / 100);
  
  return Math.min(baseBatchSize, tokenBasedBatchSize, 4096);
}

/**
 * GPU ìµœì í™” ì„¤ì • ìƒì„±
 */
export function createGPUOptimizationSettings(
  systemInfo: SystemResources,
  modelName: string = 'phi3:mini'
): Record<string, any> {
  const config = generateOptimalConfig(systemInfo, 'medium');
  
  return {
    // ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    model: modelName,
    stream: false,
    
    // ì„±ëŠ¥ ìµœì í™” ì˜µì…˜ (phi3:mini ìµœì í™”)
    options: {
      // GPU ì„¤ì •
      num_gpu: config.gpuLayers > 0 ? 1 : 0,
      gpu_layers: config.gpuLayers,
      
      // ì»¨í…ìŠ¤íŠ¸ ë° ë°°ì¹˜ ì„¤ì • (phi3:mini ìµœì í™”)
      num_ctx: config.contextSize,
      num_batch: config.numBatch,
      num_thread: config.numThread,
      
      // ë©”ëª¨ë¦¬ ìµœì í™”
      use_mmap: config.useMmap,
      use_mlock: config.useMlock,
      numa: config.numa,
      flash_attention: config.flashAttention,
      low_vram: config.lowVram,
      f16_kv: true,
      logits_all: false,
      vocab_only: false,
      
      // NPU ê´€ë ¨ ì„¤ì • (Ollama í™•ì¥)
      npu_enabled: config.hybridMode,
      npu_layers: config.npuLayers,
      
      // í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì„¤ì •
      hybrid_mode: config.hybridMode,
      
      // í’ˆì§ˆ ì„¤ì • (phi3:mini ìµœì í™”)
      temperature: 0.7,
      top_k: 40,
      top_p: 0.95,
      repeat_penalty: 1.1,
      
      // ì•ˆì •ì„± ì„¤ì •
      stop: ["<|end|>", "###", "---", "\n\n\n"]
    }
  };
}

// ì „ì—­ ì„±ëŠ¥ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
export const globalPerformanceMonitor = new GPUPerformanceMonitor();

/**
 * GPU ìµœì í™” ì´ˆê¸°í™”
 */
export async function initializeGPUOptimization(): Promise<{
  config: GPUOptimizationConfig;
  health: any;
  monitor: GPUPerformanceMonitor;
}> {
  console.log('ğŸ® GPU ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...');
  
  const health = await checkGPUHealth();
  const systemResources: SystemResources = {
    totalMemory: 64 * 1024 * 1024 * 1024, // 64GB
    availableMemory: 32 * 1024 * 1024 * 1024, // 32GB
    cpuCores: 16,
    gpuMemory: health.memoryTotal,
    hasNPU: true, // Intel AI Boost ê°ì§€ë¨
    gpuModel: 'NVIDIA RTX 4050',
    gpuUtilization: health.utilization,
    gpuTemperature: health.temperature
  };
  
  const config = generateOptimalConfig(systemResources, 'medium');
  
  console.log('âœ… GPU ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ');
  console.log(`   ğŸ® GPU: ${systemResources.gpuModel} (${health.memoryTotal}GB)`);
  console.log(`   ğŸ§  NPU: ${systemResources.hasNPU ? 'Intel AI Boost í™œì„±í™”' : 'ë¹„í™œì„±í™”'}`);
  console.log(`   ğŸ–¥ï¸  CPU: ${systemResources.cpuCores}ì½”ì–´`);
  console.log(`   ğŸ’¾ ë©”ëª¨ë¦¬: ${Math.round(systemResources.totalMemory / 1024 / 1024 / 1024)}GB`);
  
  return { config, health, monitor: globalPerformanceMonitor };
}
