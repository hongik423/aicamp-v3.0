/**
 * í•˜ì´ë¸Œë¦¬ë“œ AI í”„ë¡œë°”ì´ë” - ì´êµì¥ì˜AIìƒë‹´ ì „ìš©
 * ë¡œì»¬ Ollama (phi3:mini) ë‹¨ì¼ ì‹œìŠ¤í…œ
 */

import { CallAIParams } from './hybrid-ai-provider.types';
import { HybridAIResponse, OllamaStatus, ServiceStatus } from './hybrid-ai-provider.types';
import { hostStatusMonitor } from '@/lib/monitoring/host-status-monitor';

export class HybridAIProvider {
  private static instance: HybridAIProvider;
  private ollamaStatus: OllamaStatus | null = null;
  private lastStatusCheck: number = 0;
  private readonly STATUS_CACHE_DURATION = 30000; // 30ì´ˆ ìºì‹œ

  private constructor() {}

  static getInstance(): HybridAIProvider {
    if (!HybridAIProvider.instance) {
      HybridAIProvider.instance = new HybridAIProvider();
    }
    return HybridAIProvider.instance;
  }

  /**
   * ë¡œì»¬ Ollama ì„œë²„ ìƒíƒœ í™•ì¸ (ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©)
   */
  private async checkOllamaStatus(): Promise<OllamaStatus> {
    const now = Date.now();
    
    // ìºì‹œëœ ìƒíƒœê°€ ìˆê³  ì•„ì§ ìœ íš¨í•œ ê²½ìš°
    if (this.ollamaStatus && (now - this.lastStatusCheck) < this.STATUS_CACHE_DURATION) {
      return this.ollamaStatus;
    }

    try {
      console.log('ğŸ” ë¡œì»¬ Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘...');
      
      const baseUrl = process.env.NEXT_PUBLIC_VERCEL_URL || 'https://aicamp.club';
      const response = await fetch(`${baseUrl}/api/ollama/status`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: AbortSignal.timeout(5000)
      });

      if (!response.ok) {
        throw new Error(`ìƒíƒœ í™•ì¸ API ì˜¤ë¥˜: ${response.status}`);
      }

      const status = await response.json();
      this.ollamaStatus = status;
      this.lastStatusCheck = now;
      
      console.log(`ğŸ“Š Ollama ìƒíƒœ ì—…ë°ì´íŠ¸: ${status.isRunning ? 'ì‹¤í–‰ ì¤‘' : 'ì¤‘ì§€'}`);
      
      return status;
      
    } catch (error) {
      console.error('âŒ Ollama ìƒíƒœ í™•ì¸ ì‹¤íŒ¨:', error);
      
      const fallbackStatus: OllamaStatus = {
        isRunning: false,
        modelAvailable: false,
        modelName: 'phi3:mini',
        lastChecked: new Date().toISOString(),
        error: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'
      };
      
      this.ollamaStatus = fallbackStatus;
      this.lastStatusCheck = now;
      
      return fallbackStatus;
    }
  }

  /**
   * ë¡œì»¬ Ollama ì„œë²„ë¥¼ í†µí•œ AI í˜¸ì¶œ
   */
  private async callLocalOllama(params: CallAIParams): Promise<string> {
    const ollamaUrl = process.env.OLLAMA_API_URL || 'http://localhost:11434';
    
    console.log(`ğŸš€ ë¡œì»¬ Ollama í˜¸ì¶œ: ${ollamaUrl}`);
    
    const response = await fetch(`${ollamaUrl}/api/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'phi3:mini',
        prompt: params.prompt || '',
        stream: false,
        options: {
          temperature: params.temperature || 0.7,
          num_predict: params.maxTokens || 2048,
          top_k: 40,
          top_p: 0.95,
          repeat_penalty: 1.1
        }
      }),
      signal: AbortSignal.timeout(300000) // 5ë¶„ íƒ€ì„ì•„ì›ƒ
    });

    if (!response.ok) {
      throw new Error(`ë¡œì»¬ Ollama ì˜¤ë¥˜: ${response.status}`);
    }

    const result = await response.json();
    return result.response || '';
  }

  /**
   * ëŒ€ì²´ AI ì„œë¹„ìŠ¤ í˜¸ì¶œ (ë¡œì»¬ Ollama ì‚¬ìš© ë¶ˆê°€ ì‹œ)
   */
  // í´ë°± ê²½ë¡œëŠ” ì œê±°í•©ë‹ˆë‹¤. ì˜¤ì§ ë¡œì»¬ Ollamaë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

  /**
   * í•˜ì´ë¸Œë¦¬ë“œ AI í˜¸ì¶œ (ë¡œì»¬ ìš°ì„ , ëŒ€ì²´ ì„œë¹„ìŠ¤ ë°±ì—…)
   */
  async callAI(params: CallAIParams): Promise<HybridAIResponse> {
    const startTime = Date.now();
    
    try {
      // ë¡œì»¬ Ollama ì„œë²„ ìƒíƒœ í™•ì¸ í›„, ë°˜ë“œì‹œ ë¡œì»¬ í˜¸ì¶œ
      const ollamaStatus = await this.checkOllamaStatus();
      if (!ollamaStatus.isRunning || !ollamaStatus.modelAvailable) {
        throw new Error(ollamaStatus.error || 'Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ');
      }

      console.log('ğŸ¯ ë¡œì»¬ Ollama ì„œë²„ ì‚¬ìš©: phi3:mini');
      const response = await this.callLocalOllama(params);
      const processingTime = Date.now() - startTime;

      return {
        response,
        source: 'local',
        processingTime,
        modelUsed: 'phi3:mini (ë¡œì»¬)',
        metadata: {
          localOllamaAvailable: true
        }
      };
      
    } catch (error) {
      console.error('âŒ í•˜ì´ë¸Œë¦¬ë“œ AI í˜¸ì¶œ ì‹¤íŒ¨:', error);
      
      const processingTime = Date.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';
      
      return {
        response: `ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: ${errorMessage}\n\ní˜¸ìŠ¤íŠ¸ ì»´í“¨í„°ì˜ ì „ì›ê³¼ Ollama(phi3:mini) ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.`,
        source: 'local',
        processingTime,
        modelUsed: 'phi3:mini (ì˜¤ë¥˜)',
        metadata: {
          localOllamaAvailable: false,
          fallbackReason: errorMessage
        }
      };
    }
  }

  /**
   * í˜„ì¬ AI ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜
   */
  async getServiceStatus(): Promise<ServiceStatus> {
    const status = await this.checkOllamaStatus();
    
    return {
      localOllamaAvailable: status.isRunning && status.modelAvailable,
      modelName: status.modelName,
      lastChecked: status.lastChecked,
      responseTime: status.responseTime,
      error: status.error
    };
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë‚´ë³´ë‚´ê¸°
export const hybridAIProvider = HybridAIProvider.getInstance();
