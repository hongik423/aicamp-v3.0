/**
 * Ollama NPU ìµœì í™” ì‹œìŠ¤í…œ - ì´êµì¥ì˜AIìƒë‹´ ì „ìš©
 * NVIDIA GPU + Intel NPU í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™”
 */

export interface OllamaNPUConfig {
  contextSize: number;
  threads: number;
  gpuLayers: number;
  npuLayers: number;
  batchSize: number;
  hybridMode: boolean;
  workloadDistribution: {
    gpu: number;
    npu: number;
    cpu: number;
  };
}

export interface SystemInfo {
  gpuMemory: number;
  npuAvailable: boolean;
  cpuCores: number;
  ramSize: number;
}

export interface Pipeline {
  name: string;
  stages: string[];
  optimization: string;
}

export class OllamaNPUConfigGenerator {
  static generateOptimalConfig(systemInfo: SystemInfo): OllamaNPUConfig {
    const config: OllamaNPUConfig = {
      contextSize: 131072,
      threads: Math.min(systemInfo.cpuCores * 2, 32),
      gpuLayers: systemInfo.gpuMemory >= 8 ? 32 : 16,
      npuLayers: systemInfo.npuAvailable ? 8 : 0,
      batchSize: 2048,
      hybridMode: systemInfo.npuAvailable,
      workloadDistribution: {
        gpu: 60,
        npu: systemInfo.npuAvailable ? 30 : 0,
        cpu: systemInfo.npuAvailable ? 10 : 40
      }
    };

    // GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¡°ì •
    if (systemInfo.gpuMemory < 8) {
      config.gpuLayers = Math.floor(systemInfo.gpuMemory * 2);
      config.batchSize = 1024;
      config.workloadDistribution.gpu = 40;
      config.workloadDistribution.cpu = 60;
    }

    // NPU ê°€ìš©ì„± ê¸°ë°˜ ì¡°ì •
    if (!systemInfo.npuAvailable) {
      config.npuLayers = 0;
      config.hybridMode = false;
      config.workloadDistribution.gpu = 70;
      config.workloadDistribution.cpu = 30;
    }

    return config;
  }
}

export class WorkloadDistributor {
  private config: OllamaNPUConfig;
  private pipeline: Pipeline;

  constructor(config: OllamaNPUConfig, pipeline: Pipeline) {
    this.config = config;
    this.pipeline = pipeline;
  }

  distributeWorkload(input: any): any {
    console.log('âš–ï¸ ì›Œí¬ë¡œë“œ ë¶„ì‚° ì²˜ë¦¬ ì¤‘...');
    console.log(`   ğŸ® GPU: ${this.config.workloadDistribution.gpu}%`);
    console.log(`   ğŸ§  NPU: ${this.config.workloadDistribution.npu}%`);
    console.log(`   ğŸ–¥ï¸  CPU: ${this.config.workloadDistribution.cpu}%`);
    
    return input; // ì‹¤ì œë¡œëŠ” ë¶„ì‚° ì²˜ë¦¬ëœ ê²°ê³¼ ë°˜í™˜
  }
}

export function getOptimalPipeline(): Pipeline {
  return {
    name: 'ì´êµì¥ì˜AIìƒë‹´-í•˜ì´ë¸Œë¦¬ë“œ',
    stages: ['preprocessing', 'inference', 'postprocessing'],
    optimization: 'gpu-npu-hybrid'
  };
}

export function getOptimalBatchSize(gpuMemoryBytes: number, maxTokens: number, modelSizeGB: number): number {
  const gpuMemoryGB = gpuMemoryBytes / (1024 * 1024 * 1024);
  const availableMemory = gpuMemoryGB * 0.8; // 80% ì‚¬ìš©
  
  // ëª¨ë¸ í¬ê¸°ì™€ í† í° ìˆ˜ë¥¼ ê³ ë ¤í•œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
  const baseBatchSize = Math.floor(availableMemory / (modelSizeGB * 0.1));
  const tokenBasedBatchSize = Math.floor(maxTokens / 100);
  
  return Math.min(baseBatchSize, tokenBasedBatchSize, 4096);
}

export class HybridPerformanceMonitor {
  private metrics: {
    gpu: { usage: number; temperature: number; memory: number };
    npu: { usage: number; temperature: number; efficiency: number };
    cpu: { usage: number; temperature: number };
  } = {
    gpu: { usage: 0, temperature: 0, memory: 0 },
    npu: { usage: 0, temperature: 0, efficiency: 0 },
    cpu: { usage: 0, temperature: 0 }
  };

  updateMetrics(component: 'gpu' | 'npu' | 'cpu', data: any): void {
    this.metrics[component] = { ...this.metrics[component], ...data };
  }

  generateReport(): string {
    return `
ğŸ§  ì´êµì¥ì˜AIìƒë‹´ í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ë¦¬í¬íŠ¸:
   ğŸ® GPU: ${Math.round(this.metrics.gpu.usage)}% ì‚¬ìš©ë¥ , ${Math.round(this.metrics.gpu.temperature)}Â°C
   ğŸ§  NPU: ${Math.round(this.metrics.npu.usage)}% ì‚¬ìš©ë¥ , ${Math.round(this.metrics.npu.efficiency)}% íš¨ìœ¨ì„±
   ğŸ–¥ï¸  CPU: ${Math.round(this.metrics.cpu.usage)}% ì‚¬ìš©ë¥ , ${Math.round(this.metrics.cpu.temperature)}Â°C
    `;
  }

  checkPerformanceAlerts(): string[] {
    const alerts: string[] = [];
    
    if (this.metrics.gpu.temperature > 80) {
      alerts.push('GPU ì˜¨ë„ê°€ ë†’ìŠµë‹ˆë‹¤ (80Â°C ì´ˆê³¼)');
    }
    
    if (this.metrics.gpu.usage > 95) {
      alerts.push('GPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ (95% ì´ˆê³¼)');
    }
    
    if (this.metrics.npu.efficiency < 80) {
      alerts.push('NPU íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (80% ë¯¸ë§Œ)');
    }
    
    return alerts;
  }
}

export class NPUBenchmark {
  static async runBenchmark(): Promise<{
    npu: { latency: number; throughput: number };
    gpu: { latency: number; throughput: number };
    cpu: { latency: number; throughput: number };
  }> {
    // ë²¤ì¹˜ë§ˆí¬ ì‹œë®¬ë ˆì´ì…˜
    return {
      npu: { latency: Math.random() * 20 + 10, throughput: Math.random() * 50 + 100 },
      gpu: { latency: Math.random() * 30 + 20, throughput: Math.random() * 100 + 200 },
      cpu: { latency: Math.random() * 100 + 50, throughput: Math.random() * 20 + 50 }
    };
  }
}
