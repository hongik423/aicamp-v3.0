/**
 * Ollama GPT-OSS 20B + NPU ìµœì í™” ì‹œìŠ¤í…œ
 * Intel AI Boost NPUì™€ NVIDIA GPU í•˜ì´ë¸Œë¦¬ë“œ ê°€ì†
 * ì´êµì¥ì˜AIìƒë‹´ ì „ìš© ìµœì í™”
 */

export interface OllamaNPUConfig {
  model: string;
  gpuLayers: number;
  npuLayers: number;
  contextSize: number;
  batchSize: number;
  threads: number;
  hybridMode: boolean;
  workloadDistribution: {
    gpu: number;    // GPU ì²˜ë¦¬ ë¹„ìœ¨ (0-100)
    npu: number;    // NPU ì²˜ë¦¬ ë¹„ìœ¨ (0-100)
    cpu: number;    // CPU ì²˜ë¦¬ ë¹„ìœ¨ (0-100)
  };
}

export interface ProcessingPipeline {
  preprocessing: 'npu' | 'cpu';
  tokenization: 'npu' | 'cpu';
  embedding: 'gpu' | 'npu';
  attention: 'gpu' | 'npu';
  feedforward: 'gpu' | 'npu';
  postprocessing: 'npu' | 'cpu';
}

/**
 * Ollama NPU ìµœì í™” ì„¤ì • ìƒì„±
 */
export function generateOllamaNPUConfig(): OllamaNPUConfig {
  return {
    model: 'gpt-oss:20b',
    gpuLayers: 20,        // GPUì—ì„œ ì²˜ë¦¬í•  ë ˆì´ì–´ ìˆ˜
    npuLayers: 4,         // NPUì—ì„œ ì²˜ë¦¬í•  ë ˆì´ì–´ ìˆ˜ (ì „ì²˜ë¦¬/í›„ì²˜ë¦¬)
    contextSize: 32768,   // 32K ì»¨í…ìŠ¤íŠ¸ (ì•ˆì •ì„±)
    batchSize: 1024,      // ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
    threads: 16,          // CPU ìŠ¤ë ˆë“œ ìˆ˜
    hybridMode: true,     // GPU + NPU í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
    workloadDistribution: {
      gpu: 70,    // GPUê°€ ì£¼ìš” ì¶”ë¡  ë‹´ë‹¹
      npu: 25,    // NPUê°€ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ë‹´ë‹¹
      cpu: 5      // CPUê°€ ìŠ¤ì¼€ì¤„ë§ ë‹´ë‹¹
    }
  };
}

/**
 * ìµœì í™”ëœ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì •
 */
export function getOptimalPipeline(): ProcessingPipeline {
  return {
    preprocessing: 'npu',    // Intel AI Boostë¡œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    tokenization: 'npu',     // NPU í† í°í™” ê°€ì†
    embedding: 'gpu',        // GPU ì„ë² ë”© ì²˜ë¦¬
    attention: 'gpu',        // GPU ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
    feedforward: 'gpu',      // GPU í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
    postprocessing: 'npu'    // NPU í›„ì²˜ë¦¬ ê°€ì†
  };
}

/**
 * ë™ì  ì›Œí¬ë¡œë“œ ë¶„ì‚°ê¸°
 */
export class WorkloadDistributor {
  private config: OllamaNPUConfig;
  private pipeline: ProcessingPipeline;
  private performanceHistory: {
    gpu: number[];
    npu: number[];
    cpu: number[];
  } = { gpu: [], npu: [], cpu: [] };

  constructor(config: OllamaNPUConfig, pipeline: ProcessingPipeline) {
    this.config = config;
    this.pipeline = pipeline;
  }

  /**
   * ì‹¤ì‹œê°„ ì›Œí¬ë¡œë“œ ë¶„ì‚° ìµœì í™”
   */
  async optimizeWorkload(taskType: keyof ProcessingPipeline, taskSize: number): Promise<{
    device: 'gpu' | 'npu' | 'cpu';
    estimatedTime: number;
    confidence: number;
  }> {
    const baseDevice = this.pipeline[taskType];
    
    // ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ìµœì í™”
    const gpuLoad = this.getAverageLoad('gpu');
    const npuLoad = this.getAverageLoad('npu');
    
    // ë™ì  ì¬ë¶„ì‚° ë¡œì§
    if (baseDevice === 'gpu' && gpuLoad > 90) {
      // GPU ê³¼ë¶€í•˜ ì‹œ NPUë¡œ ì¼ë¶€ ì‘ì—… ì´ì „
      if (taskType === 'embedding' || taskType === 'attention') {
        return {
          device: 'npu',
          estimatedTime: this.estimateProcessingTime('npu', taskSize),
          confidence: 0.8
        };
      }
    }

    if (baseDevice === 'npu' && npuLoad > 85) {
      // NPU ê³¼ë¶€í•˜ ì‹œ CPUë¡œ í´ë°±
      return {
        device: 'cpu',
        estimatedTime: this.estimateProcessingTime('cpu', taskSize),
        confidence: 0.6
      };
    }

    return {
      device: baseDevice as 'gpu' | 'npu' | 'cpu',
      estimatedTime: this.estimateProcessingTime(baseDevice as 'gpu' | 'npu' | 'cpu', taskSize),
      confidence: 0.95
    };
  }

  private getAverageLoad(device: 'gpu' | 'npu' | 'cpu'): number {
    const history = this.performanceHistory[device];
    if (history.length === 0) return 0;
    return history.reduce((sum, load) => sum + load, 0) / history.length;
  }

  private estimateProcessingTime(device: 'gpu' | 'npu' | 'cpu', taskSize: number): number {
    // ì¥ì¹˜ë³„ ì²˜ë¦¬ ì†ë„ ì¶”ì • (ms)
    const processingSpeed = {
      gpu: 0.1,    // GPU: ë§¤ìš° ë¹ ë¦„
      npu: 0.3,    // NPU: ë¹ ë¦„ (íŠ¹ì • ì‘ì—…ì— íŠ¹í™”)
      cpu: 1.0     // CPU: ê¸°ë³¸ ì†ë„
    };

    return taskSize * processingSpeed[device];
  }

  /**
   * ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
   */
  updatePerformance(device: 'gpu' | 'npu' | 'cpu', load: number) {
    this.performanceHistory[device].push(load);
    
    // íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 100ê°œ)
    if (this.performanceHistory[device].length > 100) {
      this.performanceHistory[device].shift();
    }
  }
}

/**
 * NPU ê°€ì† Ollama ì„¤ì • ìƒì„±ê¸°
 */
export class OllamaNPUConfigGenerator {
  /**
   * ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ìµœì  ì„¤ì • ìƒì„±
   */
  static generateOptimalConfig(systemInfo: {
    gpuMemory: number;
    npuAvailable: boolean;
    cpuCores: number;
    ramSize: number;
  }): OllamaNPUConfig {
    const baseConfig = generateOllamaNPUConfig();

    // GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë ˆì´ì–´ ë¶„ì‚° ì¡°ì •
    if (systemInfo.gpuMemory < 8) {
      // 8GB ë¯¸ë§Œ: NPU í™œìš©ë„ ì¦ê°€
      baseConfig.gpuLayers = 12;
      baseConfig.npuLayers = 8;
      baseConfig.workloadDistribution = { gpu: 50, npu: 40, cpu: 10 };
    } else if (systemInfo.gpuMemory >= 16) {
      // 16GB ì´ìƒ: GPU ì¤‘ì‹¬ ì²˜ë¦¬
      baseConfig.gpuLayers = 22;
      baseConfig.npuLayers = 2;
      baseConfig.workloadDistribution = { gpu: 80, npu: 15, cpu: 5 };
    }

    // NPU ë¯¸ì§€ì› ì‹œ CPU í´ë°±
    if (!systemInfo.npuAvailable) {
      baseConfig.npuLayers = 0;
      baseConfig.hybridMode = false;
      baseConfig.workloadDistribution = { gpu: 85, npu: 0, cpu: 15 };
    }

    // CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ìŠ¤ë ˆë“œ ì¡°ì •
    baseConfig.threads = Math.min(systemInfo.cpuCores, 16);

    return baseConfig;
  }

  /**
   * Ollama ì‹¤í–‰ íŒŒë¼ë¯¸í„° ìƒì„±
   */
  static generateOllamaParams(config: OllamaNPUConfig): Record<string, any> {
    return {
      // ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
      model: config.model,
      stream: false,
      
      // ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
      options: {
        // GPU ì„¤ì •
        num_gpu: config.gpuLayers > 0 ? 1 : 0,
        gpu_layers: config.gpuLayers,
        
        // ì»¨í…ìŠ¤íŠ¸ ë° ë°°ì¹˜ ì„¤ì •
        num_ctx: config.contextSize,
        num_batch: config.batchSize,
        num_thread: config.threads,
        
        // ë©”ëª¨ë¦¬ ìµœì í™”
        use_mmap: true,
        use_mlock: false,  // ì•ˆì •ì„± ìš°ì„ 
        
        // NPU ê´€ë ¨ ì„¤ì • (Ollama í™•ì¥)
        npu_enabled: config.hybridMode,
        npu_layers: config.npuLayers,
        
        // í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì„¤ì •
        hybrid_mode: config.hybridMode,
        workload_gpu: config.workloadDistribution.gpu,
        workload_npu: config.workloadDistribution.npu,
        workload_cpu: config.workloadDistribution.cpu,
        
        // í’ˆì§ˆ ì„¤ì •
        temperature: 0.7,
        top_k: 40,
        top_p: 0.95,
        repeat_penalty: 1.1,
        
        // ì•ˆì •ì„± ì„¤ì •
        low_vram: config.gpuLayers < 16,
        f16_kv: true,
        logits_all: false,
        vocab_only: false
      }
    };
  }
}

/**
 * NPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
 */
export class NPUBenchmark {
  /**
   * NPU vs GPU vs CPU ì„±ëŠ¥ ë¹„êµ
   */
  static async runBenchmark(): Promise<{
    npu: { latency: number; throughput: number; efficiency: number };
    gpu: { latency: number; throughput: number; efficiency: number };
    cpu: { latency: number; throughput: number; efficiency: number };
  }> {
    console.log('ğŸ§ª NPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...');

    // ì‹œë®¬ë ˆì´ì…˜ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    return {
      npu: {
        latency: 15,      // 15ms í‰ê·  ì§€ì—°ì‹œê°„
        throughput: 1200, // 1200 tokens/sec
        efficiency: 95    // 95% ì „ë ¥ íš¨ìœ¨ì„±
      },
      gpu: {
        latency: 8,       // 8ms í‰ê·  ì§€ì—°ì‹œê°„
        throughput: 2500, // 2500 tokens/sec
        efficiency: 70    // 70% ì „ë ¥ íš¨ìœ¨ì„±
      },
      cpu: {
        latency: 45,      // 45ms í‰ê·  ì§€ì—°ì‹œê°„
        throughput: 400,  // 400 tokens/sec
        efficiency: 60    // 60% ì „ë ¥ íš¨ìœ¨ì„±
      }
    };
  }

  /**
   * ìµœì  ì²˜ë¦¬ ì¥ì¹˜ ì¶”ì²œ
   */
  static recommendOptimalDevice(taskType: string, taskSize: number): 'gpu' | 'npu' | 'cpu' {
    // ì‘ì—… ìœ í˜•ë³„ ìµœì  ì¥ì¹˜ ì¶”ì²œ
    const recommendations: Record<string, 'gpu' | 'npu' | 'cpu'> = {
      'text-generation': 'gpu',      // í…ìŠ¤íŠ¸ ìƒì„±: GPU ìµœì 
      'preprocessing': 'npu',        // ì „ì²˜ë¦¬: NPU ìµœì 
      'tokenization': 'npu',         // í† í°í™”: NPU ìµœì 
      'embedding': 'gpu',            // ì„ë² ë”©: GPU ìµœì 
      'attention': 'gpu',            // ì–´í…ì…˜: GPU ìµœì 
      'postprocessing': 'npu',       // í›„ì²˜ë¦¬: NPU ìµœì 
      'classification': 'npu',       // ë¶„ë¥˜: NPU ìµœì 
      'sentiment-analysis': 'npu'    // ê°ì • ë¶„ì„: NPU ìµœì 
    };

    return recommendations[taskType] || 'gpu';
  }
}

/**
 * ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
 */
export class HybridPerformanceMonitor {
  private metrics = {
    gpu: { usage: 0, temperature: 0, memory: 0 },
    npu: { usage: 0, temperature: 0, efficiency: 0 },
    cpu: { usage: 0, temperature: 0, threads: 0 }
  };

  /**
   * ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
   */
  updateMetrics(device: 'gpu' | 'npu' | 'cpu', metrics: any) {
    this.metrics[device] = { ...this.metrics[device], ...metrics };
  }

  /**
   * ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
   */
  generateReport(): string {
    return `
ğŸ¯ ì´êµì¥ì˜AIìƒë‹´ í•˜ì´ë¸Œë¦¬ë“œ ì„±ëŠ¥ ë¦¬í¬íŠ¸:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ® NVIDIA RTX 4050 GPU:
   ğŸ“Š ì‚¬ìš©ë¥ : ${this.metrics.gpu.usage}%
   ğŸŒ¡ï¸  ì˜¨ë„: ${this.metrics.gpu.temperature}Â°C
   ğŸ’¾ ë©”ëª¨ë¦¬: ${this.metrics.gpu.memory}%

ğŸ§  Intel AI Boost NPU:
   ğŸ“Š ì‚¬ìš©ë¥ : ${this.metrics.npu.usage}%
   ğŸŒ¡ï¸  ì˜¨ë„: ${this.metrics.npu.temperature}Â°C
   âš¡ íš¨ìœ¨ì„±: ${this.metrics.npu.efficiency}%

ğŸ–¥ï¸  CPU (ë©€í‹°ì½”ì–´):
   ğŸ“Š ì‚¬ìš©ë¥ : ${this.metrics.cpu.usage}%
   ğŸŒ¡ï¸  ì˜¨ë„: ${this.metrics.cpu.temperature}Â°C
   ğŸ§µ í™œì„± ìŠ¤ë ˆë“œ: ${this.metrics.cpu.threads}ê°œ

ğŸš€ ìµœì í™” ìƒíƒœ: Ollama GPT-OSS 20B + NPU í•˜ì´ë¸Œë¦¬ë“œ ê°€ì† í™œì„±í™”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    `;
  }

  /**
   * ì„±ëŠ¥ ê²½ê³  ì²´í¬
   */
  checkPerformanceAlerts(): string[] {
    const alerts: string[] = [];

    if (this.metrics.gpu.temperature > 80) {
      alerts.push('âš ï¸  GPU ì˜¨ë„ê°€ ë†’ìŠµë‹ˆë‹¤. ì¿¨ë§ì„ í™•ì¸í•˜ì„¸ìš”.');
    }

    if (this.metrics.gpu.usage > 95) {
      alerts.push('ğŸ’¡ GPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. NPUë¡œ ì¼ë¶€ ì‘ì—…ì„ ì´ì „í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.');
    }

    if (this.metrics.npu.efficiency < 70) {
      alerts.push('ğŸ”§ NPU íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì›Œí¬ë¡œë“œ ë¶„ì‚°ì„ ì¡°ì •í•˜ì„¸ìš”.');
    }

    return alerts;
  }
}
