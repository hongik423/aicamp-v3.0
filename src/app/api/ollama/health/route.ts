import { NextRequest, NextResponse } from 'next/server';

export const dynamic = 'force-dynamic';

export async function GET(request: NextRequest) {
  try {
    const ollamaUrl = process.env.OLLAMA_API_URL || 'http://localhost:11434';
    const model = process.env.OLLAMA_MODEL || 'gpt-oss:20b';
    
    console.log('ğŸ” Ollama ì„œë²„ í—¬ìŠ¤ì²´í¬ ì‹œì‘:', ollamaUrl);
    
    // 1. Ollama ì„œë²„ ì—°ê²° í™•ì¸
    const serverResponse = await fetch(`${ollamaUrl}/api/tags`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' },
      signal: AbortSignal.timeout(10000) // 10ì´ˆ íƒ€ì„ì•„ì›ƒ
    });
    
    if (!serverResponse.ok) {
      throw new Error(`Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: ${serverResponse.status}`);
    }
    
    const serverData = await serverResponse.json();
    const availableModels = serverData.models || [];
    const targetModel = availableModels.find((m: any) => m.name === model);
    
    // 2. ëª¨ë¸ ìƒíƒœ í™•ì¸
    let modelStatus = 'not_found';
    if (targetModel) {
      modelStatus = 'available';
      
      // 3. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
      try {
        const testResponse = await fetch(`${ollamaUrl}/api/generate`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: model,
            prompt: 'ì•ˆë…•í•˜ì„¸ìš”. ì´êµì¥ì˜AIìƒë‹´ì…ë‹ˆë‹¤.',
            stream: false,
            options: {
              temperature: 0.7,
              num_predict: 50,
              top_k: 40,
              top_p: 0.95
            }
          }),
          signal: AbortSignal.timeout(30000) // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        });
        
        if (testResponse.ok) {
          const testData = await testResponse.json();
          modelStatus = 'working';
        } else {
          modelStatus = 'error';
        }
      } catch (testError) {
        modelStatus = 'test_failed';
      }
    }
    
    // 4. ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    const systemInfo = {
      ollamaUrl,
      targetModel: model,
      availableModels: availableModels.map((m: any) => m.name),
      modelStatus,
      timestamp: new Date().toISOString(),
      environment: process.env.NODE_ENV || 'development',
      version: process.env.VERSION || 'V16.0-OLLAMA-ULTIMATE'
    };
    
    console.log('âœ… Ollama í—¬ìŠ¤ì²´í¬ ì™„ë£Œ:', systemInfo);
    
    return NextResponse.json({
      success: true,
      status: 'healthy',
      data: systemInfo,
      message: 'ì´êµì¥ì˜AIìƒë‹´ Ollama GPT-OSS 20B ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.'
    });
    
  } catch (error: any) {
    console.error('âŒ Ollama í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨:', error);
    
    return NextResponse.json({
      success: false,
      status: 'unhealthy',
      error: error.message,
      message: 'Ollama ì„œë²„ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.',
      troubleshooting: {
        checkServer: 'Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (ollama serve)',
        checkModel: 'GPT-OSS 20B ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ollama pull gpt-oss:20b)',
        checkPort: 'í¬íŠ¸ 11434ê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸',
        contactAdmin: 'ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜: 010-9251-9743'
      }
    }, { status: 503 });
  }
}
